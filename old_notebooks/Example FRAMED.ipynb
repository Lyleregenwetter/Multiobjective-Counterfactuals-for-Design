{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b496134c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris, fetch_california_housing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "import decode_mcd.multi_objective_problem as MOP\n",
    "from decode_mcd.counterfactuals_generator import CounterfactualsGenerator\n",
    "from decode_mcd import data_package\n",
    "import load_data\n",
    "import importlib\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0afaa96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, _, _ = load_data.load_framed_dataset(\"r\", onehot = True, scaled = False, augmented = True)\n",
    "y = y.loc[:,[\"Model Mass Magnitude\", \"Sim 1 Safety Factor (Inverted)\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaea50fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from autogluon.common.utils.utils import setup_outputdir\n",
    "from autogluon.core.utils.loaders import load_pkl\n",
    "from autogluon.core.utils.savers import save_pkl\n",
    "import os.path\n",
    "\n",
    "class MultilabelPredictor():\n",
    "    \"\"\" Tabular Predictor for predicting multiple columns in table.\n",
    "        Creates multiple TabularPredictor objects which you can also use individually.\n",
    "        You can access the TabularPredictor for a particular label via: `multilabel_predictor.get_predictor(label_i)`\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        labels : List[str]\n",
    "            The ith element of this list is the column (i.e. `label`) predicted by the ith TabularPredictor stored in this object.\n",
    "        path : str, default = None\n",
    "            Path to directory where models and intermediate outputs should be saved.\n",
    "            If unspecified, a time-stamped folder called \"AutogluonModels/ag-[TIMESTAMP]\" will be created in the working directory to store all models.\n",
    "            Note: To call `fit()` twice and save all results of each fit, you must specify different `path` locations or don't specify `path` at all.\n",
    "            Otherwise files from first `fit()` will be overwritten by second `fit()`.\n",
    "            Caution: when predicting many labels, this directory may grow large as it needs to store many TabularPredictors.\n",
    "        problem_types : List[str], default = None\n",
    "            The ith element is the `problem_type` for the ith TabularPredictor stored in this object.\n",
    "        eval_metrics : List[str], default = None\n",
    "            The ith element is the `eval_metric` for the ith TabularPredictor stored in this object.\n",
    "        consider_labels_correlation : bool, default = True\n",
    "            Whether the predictions of multiple labels should account for label correlations or predict each label independently of the others.\n",
    "            If True, the ordering of `labels` may affect resulting accuracy as each label is predicted conditional on the previous labels appearing earlier in this list (i.e. in an auto-regressive fashion).\n",
    "            Set to False if during inference you may want to individually use just the ith TabularPredictor without predicting all the other labels.\n",
    "        kwargs :\n",
    "            Arguments passed into the initialization of each TabularPredictor.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    multi_predictor_file = 'multilabel_predictor.pkl'\n",
    "\n",
    "    def __init__(self, labels, path=None, problem_types=None, eval_metrics=None, consider_labels_correlation=True, **kwargs):\n",
    "        if len(labels) < 2:\n",
    "            raise ValueError(\"MultilabelPredictor is only intended for predicting MULTIPLE labels (columns), use TabularPredictor for predicting one label (column).\")\n",
    "        if (problem_types is not None) and (len(problem_types) != len(labels)):\n",
    "            raise ValueError(\"If provided, `problem_types` must have same length as `labels`\")\n",
    "        if (eval_metrics is not None) and (len(eval_metrics) != len(labels)):\n",
    "            raise ValueError(\"If provided, `eval_metrics` must have same length as `labels`\")\n",
    "        self.path = setup_outputdir(path, warn_if_exist=False)\n",
    "        self.labels = labels\n",
    "        self.consider_labels_correlation = consider_labels_correlation\n",
    "        self.predictors = {}  # key = label, value = TabularPredictor or str path to the TabularPredictor for this label\n",
    "        if eval_metrics is None:\n",
    "            self.eval_metrics = {}\n",
    "        else:\n",
    "            self.eval_metrics = {labels[i] : eval_metrics[i] for i in range(len(labels))}\n",
    "        problem_type = None\n",
    "        eval_metric = None\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            path_i = self.path + \"Predictor_\" + label\n",
    "            if problem_types is not None:\n",
    "                problem_type = problem_types[i]\n",
    "            if eval_metrics is not None:\n",
    "                eval_metric = eval_metrics[i]\n",
    "            self.predictors[label] = TabularPredictor(label=label, problem_type=problem_type, eval_metric=eval_metric, path=path_i, **kwargs)\n",
    "\n",
    "    def fit(self, train_data, tuning_data=None, **kwargs):\n",
    "        \"\"\" Fits a separate TabularPredictor to predict each of the labels.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            train_data, tuning_data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
    "                See documentation for `TabularPredictor.fit()`.\n",
    "            kwargs :\n",
    "                Arguments passed into the `fit()` call for each TabularPredictor.\n",
    "        \"\"\"\n",
    "        if isinstance(train_data, str):\n",
    "            train_data = TabularDataset(train_data)\n",
    "        if tuning_data is not None and isinstance(tuning_data, str):\n",
    "            tuning_data = TabularDataset(tuning_data)\n",
    "        train_data_og = train_data.copy()\n",
    "        if tuning_data is not None:\n",
    "            tuning_data_og = tuning_data.copy()\n",
    "        else:\n",
    "            tuning_data_og = None\n",
    "        save_metrics = len(self.eval_metrics) == 0\n",
    "        for i in range(len(self.labels)):\n",
    "            label = self.labels[i]\n",
    "            predictor = self.get_predictor(label)\n",
    "            if not self.consider_labels_correlation:\n",
    "                labels_to_drop = [l for l in self.labels if l != label]\n",
    "            else:\n",
    "                labels_to_drop = [self.labels[j] for j in range(i+1, len(self.labels))]\n",
    "            train_data = train_data_og.drop(labels_to_drop, axis=1)\n",
    "            if tuning_data is not None:\n",
    "                tuning_data = tuning_data_og.drop(labels_to_drop, axis=1)\n",
    "            print(f\"Fitting TabularPredictor for label: {label} ...\")\n",
    "            predictor.fit(train_data=train_data, tuning_data=tuning_data, **kwargs)\n",
    "            self.predictors[label] = predictor.path\n",
    "            if save_metrics:\n",
    "                self.eval_metrics[label] = predictor.eval_metric\n",
    "        self.save()\n",
    "\n",
    "    def predict(self, data, **kwargs):\n",
    "        \"\"\" Returns DataFrame with label columns containing predictions for each label.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
    "                Data to make predictions for. If label columns are present in this data, they will be ignored. See documentation for `TabularPredictor.predict()`.\n",
    "            kwargs :\n",
    "                Arguments passed into the predict() call for each TabularPredictor.\n",
    "        \"\"\"\n",
    "        return self._predict(data, as_proba=False, **kwargs)\n",
    "\n",
    "    def predict_proba(self, data, **kwargs):\n",
    "        \"\"\" Returns dict where each key is a label and the corresponding value is the `predict_proba()` output for just that label.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
    "                Data to make predictions for. See documentation for `TabularPredictor.predict()` and `TabularPredictor.predict_proba()`.\n",
    "            kwargs :\n",
    "                Arguments passed into the `predict_proba()` call for each TabularPredictor (also passed into a `predict()` call).\n",
    "        \"\"\"\n",
    "        return self._predict(data, as_proba=True, **kwargs)\n",
    "\n",
    "    def evaluate(self, data, **kwargs):\n",
    "        \"\"\" Returns dict where each key is a label and the corresponding value is the `evaluate()` output for just that label.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
    "                Data to evalate predictions of all labels for, must contain all labels as columns. See documentation for `TabularPredictor.evaluate()`.\n",
    "            kwargs :\n",
    "                Arguments passed into the `evaluate()` call for each TabularPredictor (also passed into the `predict()` call).\n",
    "        \"\"\"\n",
    "        data = self._get_data(data)\n",
    "        eval_dict = {}\n",
    "        for label in self.labels:\n",
    "            print(f\"Evaluating TabularPredictor for label: {label} ...\")\n",
    "            predictor = self.get_predictor(label)\n",
    "            eval_dict[label] = predictor.evaluate(data, **kwargs)\n",
    "            if self.consider_labels_correlation:\n",
    "                data[label] = predictor.predict(data, **kwargs)\n",
    "        return eval_dict\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\" Save MultilabelPredictor to disk. \"\"\"\n",
    "        for label in self.labels:\n",
    "            if not isinstance(self.predictors[label], str):\n",
    "                self.predictors[label] = self.predictors[label].path\n",
    "        save_pkl.save(path=self.path+self.multi_predictor_file, object=self)\n",
    "        print(f\"MultilabelPredictor saved to disk. Load with: MultilabelPredictor.load('{self.path}')\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        \"\"\" Load MultilabelPredictor from disk `path` previously specified when creating this MultilabelPredictor. \"\"\"\n",
    "        path = os.path.expanduser(path)\n",
    "        if path[-1] != os.path.sep:\n",
    "            path = path + os.path.sep\n",
    "        return load_pkl.load(path=path+cls.multi_predictor_file)\n",
    "\n",
    "    def get_predictor(self, label):\n",
    "        \"\"\" Returns TabularPredictor which is used to predict this label. \"\"\"\n",
    "        predictor = self.predictors[label]\n",
    "        if isinstance(predictor, str):\n",
    "            return TabularPredictor.load(path=predictor)\n",
    "        return predictor\n",
    "\n",
    "    def _get_data(self, data):\n",
    "        if isinstance(data, str):\n",
    "            return TabularDataset(data)\n",
    "        return data.copy()\n",
    "\n",
    "    def _predict(self, data, as_proba=False, **kwargs):\n",
    "        data = self._get_data(data)\n",
    "        if as_proba:\n",
    "            predproba_dict = {}\n",
    "        for label in self.labels:\n",
    "#             print(f\"Predicting with TabularPredictor for label: {label} ...\")\n",
    "            predictor = self.get_predictor(label)\n",
    "            if as_proba:\n",
    "                predproba_dict[label] = predictor.predict_proba(data, as_multiclass=True, **kwargs)\n",
    "            data[label] = predictor.predict(data, **kwargs)\n",
    "        if not as_proba:\n",
    "            return data[self.labels]\n",
    "        else:\n",
    "            return predproba_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e34d93f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data=pd.concat([x, y], axis=1)\n",
    "data_train, data_test, x_train, x_test, y_train, y_test=train_test_split(data, x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdf11e72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20230521_174408\\Predictor_Model Mass Magnitude\\\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.10.9\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Train Data Rows:    11880\n",
      "Train Data Columns: 39\n",
      "Label Column: Model Mass Magnitude\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (22.83, 0.73, 5.23366, 2.67333)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    26038.76 MB\n",
      "\tTrain Data (Original)  Memory Usage: 3.46 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 5 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 34 | ['CS Length', 'BB Drop', 'Stack', 'SS E', 'ST Angle', ...]\n",
      "\t\t('int', [])   :  5 | ['Material=Steel', 'Material=Aluminum', 'Material=Titanium', 'SSB_Include', 'CSB_Include']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 34 | ['CS Length', 'BB Drop', 'Stack', 'SS E', 'ST Angle', ...]\n",
      "\t\t('int', ['bool']) :  5 | ['Material=Steel', 'Material=Aluminum', 'Material=Titanium', 'SSB_Include', 'CSB_Include']\n",
      "\t0.1s = Fit runtime\n",
      "\t39 features in original data used to generate 39 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 3.29 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.08s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 10692, Val Rows: 1188\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TabularPredictor for label: Model Mass Magnitude ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-2.466\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.08s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-2.2882\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.449114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4453\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.55s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.464235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.463\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.44s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\t-0.6415\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.32s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-0.4247\t = Validation score   (-root_mean_squared_error)\n",
      "\t80.38s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-0.6189\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.51s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t-0.4164\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.99s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t-0.4687\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.83s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-0.4707\t = Validation score   (-root_mean_squared_error)\n",
      "\t34.13s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.477384\n",
      "[2000]\tvalid_set's rmse: 0.476957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4769\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.63s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-0.3866\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.19s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 156.5s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230521_174408\\Predictor_Model Mass Magnitude\\\")\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20230521_174408\\Predictor_Sim 1 Safety Factor (Inverted)\\\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.10.9\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Train Data Rows:    11880\n",
      "Train Data Columns: 40\n",
      "Label Column: Sim 1 Safety Factor (Inverted)\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (147.9071143321994, 0.0723589001447178, 2.05965, 3.37857)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    23910.7 MB\n",
      "\tTrain Data (Original)  Memory Usage: 3.55 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 5 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 35 | ['CS Length', 'BB Drop', 'Stack', 'SS E', 'ST Angle', ...]\n",
      "\t\t('int', [])   :  5 | ['Material=Steel', 'Material=Aluminum', 'Material=Titanium', 'SSB_Include', 'CSB_Include']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 35 | ['CS Length', 'BB Drop', 'Stack', 'SS E', 'ST Angle', ...]\n",
      "\t\t('int', ['bool']) :  5 | ['Material=Steel', 'Material=Aluminum', 'Material=Titanium', 'SSB_Include', 'CSB_Include']\n",
      "\t0.1s = Fit runtime\n",
      "\t40 features in original data used to generate 40 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 3.39 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.08s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 10692, Val Rows: 1188\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-3.4992\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-3.4354\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TabularPredictor for label: Sim 1 Safety Factor (Inverted) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t-2.7822\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.51s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t-2.7279\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.51s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\t-2.8252\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.94s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-2.7787\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.97s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-2.8886\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.63s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t-2.955\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.9s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t-3.1682\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.98s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-2.6947\t = Validation score   (-root_mean_squared_error)\n",
      "\t39.95s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t-2.8459\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.2s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-2.6542\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.2s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 69.61s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230521_174408\\Predictor_Sim 1 Safety Factor (Inverted)\\\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilabelPredictor saved to disk. Load with: MultilabelPredictor.load('AutogluonModels\\ag-20230521_174408\\')\n"
     ]
    }
   ],
   "source": [
    "labels=y.columns\n",
    "predictor = MultilabelPredictor(labels=labels)\n",
    "predictor.fit(train_data=data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b35ffdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols = x.columns\n",
    "def fn(x_f):\n",
    "    x_f = pd.DataFrame(x_f, columns = data_cols)\n",
    "    p = predictor.predict(x_f)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15aff7f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial population randomly initialized!\n",
      "Training GA from 0 to 20 generations!\n",
      "==========================================================================================\n",
      "n_gen  |  n_eval  | n_nds  |     cv_min    |     cv_avg    |      eps      |   indicator  \n",
      "==========================================================================================\n",
      "     1 |      499 |      2 |  0.000000E+00 |  1.6040000000 |             - |             -\n",
      "     2 |      999 |      3 |  0.000000E+00 |  1.1820000000 |  0.4816577589 |         ideal\n",
      "     3 |     1499 |      9 |  0.000000E+00 |  0.9700000000 |  1.0000000000 |         ideal\n",
      "     4 |     1999 |     10 |  0.000000E+00 |  0.9500000000 |  0.3166675259 |         ideal\n",
      "     5 |     2499 |     11 |  0.000000E+00 |  0.9260000000 |  0.3109350892 |         ideal\n",
      "     6 |     2999 |     14 |  0.000000E+00 |  0.8940000000 |  0.2141366762 |         ideal\n",
      "     7 |     3499 |     11 |  0.000000E+00 |  0.8400000000 |  0.0414574911 |         ideal\n",
      "     8 |     3999 |     13 |  0.000000E+00 |  0.7660000000 |  0.2447870140 |         ideal\n",
      "     9 |     4499 |     20 |  0.000000E+00 |  0.6400000000 |  0.2250360849 |         ideal\n",
      "    10 |     4999 |     28 |  0.000000E+00 |  0.4700000000 |  0.1608655851 |         ideal\n",
      "    11 |     5499 |     35 |  0.000000E+00 |  0.2000000000 |  0.0662322132 |         ideal\n",
      "    12 |     5999 |     51 |  0.000000E+00 |  0.000000E+00 |  0.1043927688 |         ideal\n",
      "    13 |     6499 |     76 |  0.000000E+00 |  0.000000E+00 |  0.0294286243 |         ideal\n",
      "    14 |     6999 |     90 |  0.000000E+00 |  0.000000E+00 |  0.0721693550 |             f\n",
      "    15 |     7499 |    106 |  0.000000E+00 |  0.000000E+00 |  0.0875831263 |         ideal\n",
      "    16 |     7999 |    122 |  0.000000E+00 |  0.000000E+00 |  0.0604509869 |         ideal\n",
      "    17 |     8499 |    126 |  0.000000E+00 |  0.000000E+00 |  0.0637891735 |         ideal\n",
      "    18 |     8999 |    142 |  0.000000E+00 |  0.000000E+00 |  0.0641900066 |         ideal\n",
      "    19 |     9499 |    177 |  0.000000E+00 |  0.000000E+00 |  0.0214331343 |         ideal\n",
      "    20 |     9999 |    203 |  0.000000E+00 |  0.000000E+00 |  0.0767062893 |         ideal\n"
     ]
    }
   ],
   "source": [
    "from pymoo.core.variable import Real, Integer, Binary, Choice\n",
    "from decode_mcd.design_targets import *\n",
    "\"Model Mass Magnitude\", \"Sim 1 Safety Factor (Inverted)\"\n",
    "\n",
    "design_targets = DesignTargets([ContinuousTarget(\"Model Mass Magnitude\", 0, 5), \n",
    "                              ContinuousTarget(\"Sim 1 Safety Factor (Inverted)\", 0, 1)])\n",
    "bonus_objs = [\"Model Mass Magnitude\", \"Sim 1 Safety Factor (Inverted)\"]\n",
    "\n",
    "\n",
    "lbs = np.quantile(x.values, 0.01, axis=0)\n",
    "ubs = np.quantile(x.values, 0.99, axis=0)\n",
    "datatypes=[]\n",
    "for i in range(len(x.columns)):\n",
    "    datatypes.append(Real(bounds=(lbs[i], ubs[i])))\n",
    "\n",
    "query_x = x.iloc[0:1] \n",
    "dp = data_package.DataPackage(x, y, query_x, design_targets, datatypes, x.columns, bonus_objs)\n",
    "problem = MOP.MultiObjectiveProblem(dp, fn, [])\n",
    "generator = CounterfactualsGenerator(problem, 500, initialize_from_dataset=False)\n",
    "generator.generate(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c0f9a95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting all counterfactual candidates!\n",
      "Scoring all counterfactual candidates!\n",
      "Calculating diversity matrix!\n",
      "Sampling diverse set of counterfactual candidates!\n",
      "Done! Returning CFs\n",
      "   Material=Steel  Material=Aluminum  Material=Titanium  SSB_Include  \\\n",
      "0        1.000000           0.027727           0.021178     0.002650   \n",
      "1        0.380201           0.877051           0.048397     0.000000   \n",
      "2        0.066706           0.309734           0.013792     0.754630   \n",
      "3        0.891239           0.038914           0.010385     0.495558   \n",
      "4        0.976761           0.030608           0.017376     0.369496   \n",
      "5        0.651423           0.124726           0.434200     0.015142   \n",
      "6        0.946584           0.000000           0.004792     0.000000   \n",
      "7        0.500876           0.048408           0.000415     0.067897   \n",
      "8        0.009159           0.000000           0.000118     0.000000   \n",
      "9        0.340405           0.044520           0.000544     0.124289   \n",
      "\n",
      "   CSB_Include  CS Length   BB Drop     Stack      SS E   ST Angle  ...  \\\n",
      "0     0.120150   0.328631  0.032419  0.563843  0.023362  72.500000  ...   \n",
      "1     0.686167   0.327817  0.025648  0.565600  0.045000  70.518620  ...   \n",
      "2     0.008335   0.354949 -0.032579  0.553969  0.039505  77.748726  ...   \n",
      "3     0.357023   0.325033 -0.030182  0.564718  0.136036  72.477637  ...   \n",
      "4     0.361810   0.327781  0.024061  0.565017  0.037036  75.024474  ...   \n",
      "5     0.504548   0.343903 -0.028621  0.634256  0.044828  76.161876  ...   \n",
      "6     0.020975   0.430000  0.031811  0.565600  0.045000  72.500000  ...   \n",
      "7     0.000000   0.351170  0.019623  0.565647  0.108492  72.301067  ...   \n",
      "8     0.365029   0.430000  0.019688  0.565600  0.132343  72.500000  ...   \n",
      "9     0.382676   0.334281  0.009527  0.565505  0.087284  66.493484  ...   \n",
      "\n",
      "   CSB Offset      SS Z  SS Thickness  CS Thickness  TT Thickness  \\\n",
      "0    0.350000  0.007253      0.000782      0.001494      0.002085   \n",
      "1    0.351470  0.011441      0.004851      0.001389      0.004275   \n",
      "2    0.303964  0.011698      0.001130      0.000806      0.005031   \n",
      "3    0.345417  0.007062      0.009165      0.002218      0.005007   \n",
      "4    0.349872  0.011318      0.004830      0.000996      0.001859   \n",
      "5    0.292639  0.009066      0.004824      0.001363      0.002533   \n",
      "6    0.350000  0.009345      0.007763      0.001547      0.002192   \n",
      "7    0.374806  0.009041      0.004969      0.001488      0.002192   \n",
      "8    0.348637  0.007296      0.002182      0.001508      0.002192   \n",
      "9    0.327918  0.010536      0.004891      0.001331      0.001894   \n",
      "\n",
      "   BB Thickness  HT Thickness  ST Thickness  DT Thickness  DT Length  \n",
      "0      0.007481      0.003089      0.002080      0.001295   0.586332  \n",
      "1      0.008157      0.010045      0.006755      0.001295   0.535236  \n",
      "2      0.008994      0.000413      0.002072      0.003123   0.524244  \n",
      "3      0.001118      0.001159      0.002019      0.002475   0.579433  \n",
      "4      0.001124      0.002193      0.003513      0.002936   0.541569  \n",
      "5      0.008119      0.005524      0.005996      0.003723   0.518566  \n",
      "6      0.001248      0.001223      0.002080      0.003049   0.664021  \n",
      "7      0.003620      0.000862      0.002083      0.004787   0.560218  \n",
      "8      0.003824      0.004916      0.002080      0.002798   0.664021  \n",
      "9      0.001273      0.004697      0.001969      0.006141   0.529689  \n",
      "\n",
      "[10 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "num_samples = 10\n",
    "cfs = generator.sample_with_dtai(num_samples, 0.5, 0.2, 0.5, 0.2, np.array([1,0.5]), include_dataset=False, num_dpp=10000)\n",
    "print(cfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73279aab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
